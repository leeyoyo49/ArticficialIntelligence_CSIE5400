{
  "title": "BERTScore Architecture",
  "definitions": {
    "Contextual Embeddings": "Reference and candidate sentences are represented using contextual embeddings based on surrounding words, computed by models like BERT, RoBERTa, XLNET, and XLM.",
    "Pairwise Cosine Similarity": "A measure of similarity between two vectors that is based on the cosine of the angle between them.",
    "Maximum Similarity": "The highest similarity score between the reference and candidate sentences.",
    "Importance Weighting (Optional)": "A method to adjust the importance of different parts of the sentence based on their contribution to the overall similarity score."
  },
  "formulas": [
    "R_BERT = (0.713 x 1.27) + (0.515 x 7.94) + ... / (1.27 + 7.94 + 1.82 + 7.90 + 8.88)",
    "Pairwise Cosine Similarity Formula: cos(Î¸) = (A . B) / (||A|| ||B||)",
    "Maximum Similarity Formula: max(Similarity Scores)"
  ],
  "keywords": [
    "BERT",
    "RoBERTa",
    "XLNET",
    "XLM",
    "Contextual Embeddings"
  ],
  "summary": "The slide presents the BERTScore Architecture, explaining how it uses contextual embeddings from models like BERT, RoBERTa, XLNET, and XLM to represent sentences. It details the process of calculating pairwise cosine similarity between reference and candidate sentences, identifying the maximum similarity, and optionally applying importance weighting to adjust the scores."
}
