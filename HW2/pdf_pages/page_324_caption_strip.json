{
  "title": "Recap: Defining MDPs",
  "definitions": {
    "Markov decision processes": "A framework for decision-making in a stochastic environment where the outcome is partly under the control of a decision maker and partly random.",
    "Set of states S": "A collection of all possible states in the environment.",
    "Start state s0": "The initial state from which the decision process begins.",
    "Set of actions A": "A collection of all possible actions that can be taken in each state.",
    "Transitions P(s'|s,a)": "The probability of transitioning from state s to state s' given action a.",
    "Rewards R(s,a,s')": "The reward received after taking action a in state s and ending in state s'.",
    "Discount Î³": "A factor that determines the present value of future rewards."
  },
  "formulas": [
    "Policy = Choice of action for each state",
    "Utility = sum of (discounted) rewards"
  ],
  "keywords": [
    "Markov",
    "decision processes",
    "states",
    "actions",
    "transitions",
    "rewards",
    "discount",
    "utility",
    "policy"
  ],
  "summary": "The slide recaps the definition of Markov decision processes (MDPs), detailing the components such as states, actions, transitions, and rewards. It also introduces the concepts of policy and utility in the context of MDPs."
}