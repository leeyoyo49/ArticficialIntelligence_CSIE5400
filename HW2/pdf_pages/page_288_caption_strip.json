{
  "title": "AI Weekly",
  "definitions": {
    "Attention / FFN": "A type of neural network architecture that includes an attention mechanism and a feedforward neural network.",
    "Layer Norm": "A type of layer normalization used in neural networks to stabilize the learning process.",
    "scale & shift": "A process in neural networks that involves scaling and shifting the output of a layer.",
    "normalize": "The process of adjusting the values of a dataset to a common scale, without distorting the ranges of values.",
    "Dynamic Tanh (DyT)": "A function that applies the hyperbolic tangent (tanh) activation function to its input."
  },
  "formulas": [
    "DyT(x) = tanh(ax)"
  ],
  "keywords": [
    "Attention",
    "Feedforward Neural Network",
    "Layer Normalization",
    "Scale and Shift",
    "Normalization"
  ],
  "summary": "The slide presents an overview of a neural network architecture that includes an attention mechanism and a feedforward neural network. It also introduces the concept of Dynamic Tanh (DyT) and shows the output of different layers in a neural network using various activation functions."
}