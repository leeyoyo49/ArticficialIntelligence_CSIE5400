<|im_start|>system<|im_sep|>You are an AI lecture slide analyzer. The following input is an image and the OCR of a lecture slide about “Artificial Intelligence.”

1. Extract every piece of written content:
   • Slide title
   • Sub-bullets and their full text
   • Definitions, formulas, and any inline examples

2. The summary should be as thorough and precise as possible—this will be used for later retrieval and generation.

3. The keywords should be the most relevant terms from the slide, containing at least 5 terms.

4. Organize your output as a valid JSON object with these fields:
   {
     "title": string,
     "definitions": { term: definition },
     "formulas": [ string ],
     "keywords": [ string ],
     "summary": string
   }

5. Output ONLY the JSON object as a string—no extra text or formatting.

6. The JSON object should be valid and parsable.<|im_end|><|im_start|>assistant<|im_sep|>```json
{
  "title": "Multimodal Reasoning",
  "definitions": {
    "VPG": "Visual-Prompt Generators"
  },
  "formulas": [
    "MLLMs focus on a single image",
    "MLLMs with few-shot ability",
    "MLLMs with text-to-image reference"
  ],
  "keywords": [
    "Multimodal",
    "Reasoning",
    "VPG",
    "LLM",
    "Image Declaration"
  ],
  "summary": "The slide presents the concept of Multimodal Reasoning, focusing on Visual-Prompt Generators (VPG). It illustrates three types of Multimodal Language Models (MLLMs): those focusing on a single image, those with few-shot ability, and those with text-to-image reference. The diagram shows the flow of information from text inputs through LLMs to VPGs, which then generate images. The source of the information is cited as 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,' ICLR, 2024."
}
```