<|im_start|>system<|im_sep|>You are an AI lecture slide analyzer. The following input is an image and the OCR of a lecture slide about “Artificial Intelligence.”

1. Extract every piece of written content:
   • Slide title
   • Sub-bullets and their full text
   • Definitions, formulas, and any inline examples

2. The summary should be as thorough and precise as possible—this will be used for later retrieval and generation.

3. The keywords should be the most relevant terms from the slide, containing at least 5 terms.

4. Organize your output as a valid JSON object with these fields:
   {
     "title": string,
     "definitions": { term: definition },
     "formulas": [ string ],
     "keywords": [ string ],
     "summary": string
   }

5. Output ONLY the JSON object as a string—no extra text or formatting.

6. The JSON object should be valid and parsable.<|im_end|><|im_start|>assistant<|im_sep|>```json
{
  "title": "The Transformer Decoder Block (3)",
  "definitions": {
    "Transformer": "A type of neural network architecture that relies on self-attention mechanisms to process sequential data.",
    "Decoder": "A component of the Transformer model that decodes the input data into an output sequence.",
    "FC": "Fully Connected layer",
    "MLP": "Multi-Layer Perceptron",
    "Layer norm": "Layer Normalization",
    "Masked Multi-head self-attention": "A type of self-attention mechanism that only interacts with past inputs in a sequence model."
  },
  "formulas": [
    "y0, y1, y2, y3",
    "C0,0, C0,1, C0,2, ...",
    "C2,2"
  ],
  "keywords": [
    "Transformer",
    "Decoder",
    "FC",
    "MLP",
    "Layer norm"
  ],
  "summary": "The slide presents the third block of the Transformer Decoder, highlighting the structure and components such as the Fully Connected layer, Multi-Layer Perceptron, and Layer Normalization. It also emphasizes the use of Masked Multi-head self-attention, which only interacts with past inputs."
}
```