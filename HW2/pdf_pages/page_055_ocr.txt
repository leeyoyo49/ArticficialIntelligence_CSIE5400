iY   The RECAP Framework
ys

  

m= This work proposes to relabel the corpus with improved captions that are
auto-generated by a custom I2T (image-to-text) model, so as to increase
sample efficiency and allow a vision-language model to better understand the
relations between captions and images.

Images

   
                                                                                                                                                                                                                                                                 
 
                                                                                                                                                                                                                                                                    
                                                                                      
  

3. Train Image
Generation Model

 
 
 

   

1a. Manually                                                             2. Generate
caption a                                                              captions
sample

  
   

1b. Finetune
Captioning Model

Figure 2. Schematic diagram of our method RECAP. In steps (1a) and (1b) we fine-tune an image-to-text captioning model on a small set of
detailed human captions. In step (2) we use this fine-tuned model to recaption the images in the training dataset of a text-to-image model,
and with this dataset, in step (3) we train an image generation model with the recaptioned dataset.

55
