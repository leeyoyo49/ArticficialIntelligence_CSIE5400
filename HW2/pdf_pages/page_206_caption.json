<|im_start|>system<|im_sep|>You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.”
1. Extract every piece of written content:
   • Slide title
   • Sub-bullets and their full text
   • Definitions, formulas, and any inline examples
2. The summary should be as thorough and precise as possible—this will be used for later retrieval and generation.
3. The keywords should be the most relevant terms from the slide, containing at least 5 terms.
4. Organize your output as a valid JSON object with these fields:
   {
     "title": string,
     "definitions": { term: definition },
     "formulas": [ string ],
     "keywords": [ string ],
     "summary": string
   }<|im_end|><|im_start|>user<|im_sep|>iY     LLaVA-CoT                                                                 (
『

¢ LLaVA-CoT is a novel VLM designed to conduct autonomous
multistage reasoning, capable of generating four distinct
stages: summary, caption, reasoning, and conclusion.
Summary: A brief outline in which the model summarizes the
forthcoming task.
Caption: A description of the relevant parts of an image (if present),
focusing on elements related to the question.
Reasoning: A detailed analysis in which the model systematically
considers the question.
Conclusion: A concise summary of the answer, providing a final
response based on the preceding reasoning.

32                                                                                  Source: “LLaVA-CoT: Let Vision Language Models Reason Step-by-Step,” arXiv, 2025.<|im_end|><|im_start|>assistant<|im_sep|>{"title": "LLaVA-CoT", "definitions": {"Summary": "A brief outline in which the model summarizes the forthcoming task.", "Caption": "A description of the relevant parts of an image (if present), focusing on elements related to the question.", "Reasoning": "A detailed analysis in which the model systematically considers the question.", "Conclusion": "A concise summary of the answer, providing a final response based on the preceding reasoning."}, "formulas": [], "keywords": ["LLaVA-CoT", "VLM", "autonomous", "multistage reasoning", "summary", "caption", "reasoning", "conclusion"], "summary": "LLaVA-CoT is a novel VLM designed to conduct autonomous multistage reasoning, capable of generating four distinct stages: summary, caption, reasoning, and conclusion. The model summarizes the forthcoming task, describes relevant parts of an image, systematically considers the question, and provides a final response based on the preceding reasoning."}