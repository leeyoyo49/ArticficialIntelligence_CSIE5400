{
  "title": "The Transformer Decoder Block (3)",
  "definitions": {
    "Transformer": "A type of neural network architecture that relies on self-attention mechanisms to process sequential data.",
    "Decoder": "A component of the Transformer model that decodes the input data into an output sequence.",
    "FC": "Fully Connected layer",
    "MLP": "Multi-Layer Perceptron",
    "Layer norm": "Layer Normalization",
    "Masked Multi-head self-attention": "A type of self-attention mechanism that only interacts with past inputs in a sequence model."
  },
  "formulas": [
    "y0, y1, y2, y3",
    "C0,0, C0,1, C0,2, ...",
    "C2,2"
  ],
  "keywords": [
    "Transformer",
    "Decoder",
    "FC",
    "MLP",
    "Layer norm"
  ],
  "summary": "The slide presents the third block of the Transformer Decoder, highlighting the structure and components such as the Fully Connected layer, Multi-Layer Perceptron, and Layer Normalization. It also emphasizes the use of Masked Multi-head self-attention, which only interacts with past inputs."
}