{
  "title": "Comparison",
  "definitions": {
    "value iteration": "Both value iteration and policy iteration compute the same thing (all optimal values).",
    "policy iteration": "We do several passes that update utilities with fixed policy (each pass is fast because we consider only one action, not all of them). After the policy is evaluated, a new policy is chosen (slow like a value iteration pass). The new policy will be better (or we're done)."
  },
  "formulas": [
    "Both value iteration and policy iteration compute the same thing (all optimal values).",
    "In value iteration: Every iteration updates both the values and (implicitly) the policy. We don't track the policy, but taking the max over actions implicitly recomputes it.",
    "In policy iteration: We do several passes that update utilities with fixed policy (each pass is fast because we consider only one action, not all of them). After the policy is evaluated, a new policy is chosen (slow like a value iteration pass). The new policy will be better (or we're done)."
  ],
  "keywords": [
    "value iteration",
    "policy iteration",
    "iteration",
    "utilities",
    "policy"
  ],
  "summary": "The slide compares value iteration and policy iteration, two methods for solving Markov Decision Processes (MDPs). Both methods compute the same optimal values, but differ in their approach. Value iteration updates both values and policy implicitly, while policy iteration updates utilities with a fixed policy and chooses a new policy after evaluation."
}