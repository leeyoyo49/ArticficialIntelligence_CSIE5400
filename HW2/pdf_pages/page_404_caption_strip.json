{
  "title": "Temporal Difference Learning",
  "definitions": {
    "V(s)": "Value function for state s",
    "s": "State",
    "a": "Action",
    "s'": "Next state",
    "r": "Reward",
    "sample": "Sample of V(s)",
    "R(s,π(s))": "Reward function R(s,π(s))",
    "α": "Learning rate",
    "γ": "Discount factor"
  },
  "formulas": [
    "sample = R(s,π(s)) + γVπ(s')",
    "Vπ(s) ← (1 - α)Vπ(s) + α(sample - Vπ(s))",
    "Vπ(s) ← Vπ(s) + α(sample - Vπ(s))"
  ],
  "keywords": [
    "Temporal Difference Learning",
    "Value function",
    "State",
    "Action",
    "Next state",
    "Reward",
    "Learning rate",
    "Discount factor"
  ],
  "summary": "The slide discusses Temporal Difference Learning, emphasizing the importance of learning from every experience and updating the value function each time a transition occurs. It explains how the value function is updated using a sample of the reward function and the discount factor, with the learning rate determining the extent of the update."
}