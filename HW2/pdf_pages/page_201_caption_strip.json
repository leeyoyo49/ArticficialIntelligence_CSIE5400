{
  "title": "CoT Reasoning without Prompting",
  "definitions": {
    "CoT": "Chain-of-Thought",
    "LLMs": "Language Models",
    "prompting": "CoT prompting",
    "few-shot": "few-shot",
    "zero-shot": "zero-shot"
  },
  "formulas": [
    "3+5=8"
  ],
  "keywords": [
    "CoT",
    "LLMs",
    "prompting",
    "few-shot",
    "zero-shot"
  ],
  "summary": "The slide discusses CoT reasoning without prompting, highlighting that it can be achieved by altering the decoding process of pre-trained LLMs. It includes a question in standard QA format and a decoding step diagram showing the process of reaching the correct answer by considering alternative top-k tokens."
}