<|im_start|>system<|im_sep|>You are an AI lecture slide analyzer. The following input is an image and the OCR of a lecture slide about “Artificial Intelligence.”

1. Extract every piece of written content:
   • Slide title
   • Sub-bullets and their full text
   • Definitions, formulas, and any inline examples

2. The summary should be as thorough and precise as possible—this will be used for later retrieval and generation.

3. The keywords should be the most relevant terms from the slide, containing at least 5 terms.

4. Organize your output as a valid JSON object with these fields:
   {
     "title": string,
     "definitions": { term: definition },
     "formulas": [ string ],
     "keywords": [ string ],
     "summary": string
   }

5. Output ONLY the JSON object as a string—no extra text or formatting.

6. The JSON object should be valid and parsable.<|im_end|><|im_start|>assistant<|im_sep|>```json
{
  "title": "Multimodal Embedding Spaces",
  "definitions": {
    "Multimodal learning": "Generalization across any-to-any modalities"
  },
  "formulas": [
    "Transformer encoder",
    "Transformer decoder"
  ],
  "keywords": [
    "Multimodal",
    "Embedding",
    "Spaces",
    "Generalization",
    "Any-to-any model"
  ],
  "summary": "The slide presents the concept of Multimodal learning, which is the generalization across any-to-any modalities. It features an 'Any-to-any model' with a 'Transformer encoder' and 'Transformer decoder'. The model is applied to various modalities such as RGB, Edge, Geometric, Semantic, Text, Metadata, Feature map, and Global feature modalities. Each modality is represented with an image and a brief description. The source of the information is cited as '4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities,' NeurIPS, 2024."
}
```