{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, json\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig\n",
    "from PyPDF2 import PdfReader\n",
    "import torch\n",
    "\n",
    "# Define file paths and constants\n",
    "FILE = \"AI.pdf\"  # Path to the 463-page PDF\n",
    "output_dir = \"pdf_pages\"\n",
    "sep = \"<|im_sep|>\"\n",
    "\n",
    "# 系統提示語\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.”\n",
    "1. Extract every piece of written content:\n",
    "   • Slide title\n",
    "   • Sub-bullets and their full text\n",
    "   • Definitions, formulas, and any inline examples\n",
    "2. The summary should be as thorough and precise as possible—this will be used for later retrieval and generation.\n",
    "3. The keywords should be the most relevant terms from the slide, containing at least 5 terms.\n",
    "4. Organize your output as a valid JSON object with these fields:\n",
    "   {\n",
    "     \"title\": string,\n",
    "     \"definitions\": { term: definition },\n",
    "     \"formulas\": [ string ],\n",
    "     \"keywords\": [ string ],\n",
    "     \"summary\": string\n",
    "   }\n",
    "\"\"\"\n",
    "\n",
    "# LLM setup\n",
    "model_id = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "generation_config.max_new_tokens = 1024\n",
    "do_sample = True\n",
    "temperature = 0.1\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    _attn_implementation=\"flash_attention_2\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "def ocr_image(img: Image.Image) -> str:\n",
    "    return pytesseract.image_to_string(img, lang=\"chi_tra+eng\")\n",
    "\n",
    "\n",
    "def caption_with_phi4(img: Image.Image, system: str) -> str:\n",
    "    prompt = (\n",
    "        \"<|im_start|>system<|im_sep|>\" + system.strip() +\n",
    "        # \"<|im_end|><|im_start|>user<|im_sep|>\" + user.strip() +\n",
    "        \"<|image_1|><|im_end|><|im_start|>assistant<|im_sep|>\"\n",
    "    )\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=generation_config.max_new_tokens,\n",
    "        )\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each page\n",
    "reader = PdfReader(FILE)\n",
    "for page_num in range(1, len(reader.pages) + 1):\n",
    "    # 1. render page to image\n",
    "    images = convert_from_path(FILE, dpi=200,\n",
    "                                first_page=page_num, last_page=page_num,\n",
    "                                use_pdftocairo=True)\n",
    "    img = images[0]\n",
    "\n",
    "    # # 2. OCR\n",
    "    # ocr_text = ocr_image(img)\n",
    "\n",
    "    # 3. Generate caption & strip JSON\n",
    "    attempts = 0\n",
    "    success = False\n",
    "    raw_caption = None\n",
    "    while attempts < 3 and not success:\n",
    "        raw_caption = caption_with_phi4(img, SYSTEM_PROMPT) #, ocr_text)\n",
    "        # Try stripping JSON\n",
    "        idx = raw_caption.rfind(sep)\n",
    "        if idx != -1:\n",
    "            json_part = raw_caption[idx + len(sep):].strip()\n",
    "            try:\n",
    "                data = json.loads(json_part)\n",
    "                # success\n",
    "                success = True\n",
    "            except json.JSONDecodeError:\n",
    "                # increase tokens and retry\n",
    "                generation_config.max_new_tokens += 512\n",
    "        attempts += 1\n",
    "\n",
    "    # 4. Save raw caption\n",
    "    base = f\"page_{page_num:03d}\"\n",
    "    img.save(os.path.join(output_dir, base + \".png\"))\n",
    "    with open(os.path.join(output_dir, base + \"_caption.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_caption if raw_caption else \"\")\n",
    "\n",
    "    # 5. Save stripped JSON or log failure\n",
    "    strip_path = os.path.join(output_dir, base + \"_caption_strip.json\")\n",
    "    if success:\n",
    "        with open(strip_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✅  page_{page_num:03d} processed\")\n",
    "    else:\n",
    "        print(f\"❌  page_{page_num:03d} failed to strip JSON after retries\")\n",
    "        print(f\"Raw caption: {raw_caption}\")\n",
    "\n",
    "    # 6. cleanup\n",
    "    del img, images\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save into chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File 1: build_chromadb.py\n",
    "import os\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = \"pdf_pages\"\n",
    "DB_PATH = \"./chroma_db_task2\"\n",
    "EMBEDDINGS = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load stripped JSON captions into Documents\n",
    "docs = []\n",
    "for fname in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if not fname.endswith(\"_caption_strip.json\"):\n",
    "        continue\n",
    "    page = int(fname.split(\"_\")[1])  # expects page_###_caption_strip.json\n",
    "    path = os.path.join(OUTPUT_DIR, fname)\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    text = f\"Page {page}\\nTitle: {data['title']}\\n\\nSummary:\\n{data['summary']}\"\n",
    "    docs.append(Document(page_content=text, metadata={\"page\": page}))\n",
    "\n",
    "# Build and persist Chroma vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS)\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=DB_PATH\n",
    ")\n",
    "vectordb.persist()\n",
    "print(f\"Persisted {len(docs)} documents into {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File 2: run_queries.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Configuration\n",
    "DB_PATH      = \"./chroma_db_task2\"\n",
    "QUERIES_CSV  = \"queries.csv\"\n",
    "SUBMISSION_CSV = \"submission.csv\"\n",
    "MODEL_ID     = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "\n",
    "# 1. load persisted Chroma vector store\n",
    "vectordb  = Chroma(persist_directory=DB_PATH, embedding_function=None)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 2. set up LLM pipeline for generation\n",
    "hf_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    return_full_text=False\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "# 3. build RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 4. read queries CSV\n",
    "df      = pd.read_csv(QUERIES_CSV)\n",
    "results = []\n",
    "\n",
    "# 5. process each query and collect only ID + page\n",
    "for _, row in df.iterrows():\n",
    "    qid      = row['ID']\n",
    "    question = row['Question']\n",
    "    output   = qa({\"query\": question})\n",
    "    src_docs = output['source_documents']\n",
    "    page     = src_docs[0].metadata.get('page') if src_docs else None\n",
    "\n",
    "    results.append({\n",
    "        'ID':     qid,\n",
    "        'Answer': page\n",
    "    })\n",
    "\n",
    "# 6. save submission.csv\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_df.to_csv(SUBMISSION_CSV, index=False, encoding='utf-8-sig')\n",
    "print(f\"Wrote submission to {SUBMISSION_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
