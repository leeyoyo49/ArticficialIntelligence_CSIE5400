{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5e0454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "[(3, 'json decode error: Extra data: line 1 column 700 (char 699)'), (5, 'json decode error: Expecting value: line 1 column 1 (char 0)'), (6, 'json decode error: Unterminated string starting at: line 1 column 2323 (char 2322)'), (10, 'json decode error: Extra data: line 1 column 1318 (char 1317)'), (13, 'json decode error: Extra data: line 1 column 745 (char 744)'), (14, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (15, 'json decode error: Unterminated string starting at: line 1 column 2693 (char 2692)'), (18, 'json decode error: Extra data: line 1 column 1120 (char 1119)'), (20, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 174 (char 173)'), (21, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 174 (char 173)'), (23, 'json decode error: Invalid control character at: line 1 column 64 (char 63)'), (24, 'json decode error: Extra data: line 1 column 961 (char 960)'), (27, 'json decode error: Extra data: line 1 column 2050 (char 2049)'), (28, 'json decode error: Unterminated string starting at: line 1 column 2543 (char 2542)'), (29, 'json decode error: Unterminated string starting at: line 127 column 20 (char 2965)'), (31, 'json decode error: Extra data: line 1 column 702 (char 701)'), (34, \"json decode error: Expecting ',' delimiter: line 116 column 3 (char 2635)\"), (38, 'json decode error: Extra data: line 1 column 1333 (char 1332)'), (39, 'json decode error: Extra data: line 1 column 1352 (char 1351)'), (41, 'json decode error: Extra data: line 1 column 1058 (char 1057)'), (47, \"json decode error: Expecting ',' delimiter: line 1 column 945 (char 944)\"), (52, 'json decode error: Extra data: line 1 column 2085 (char 2084)'), (55, 'json decode error: Extra data: line 1 column 1389 (char 1388)'), (57, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (62, 'json decode error: Extra data: line 1 column 852 (char 851)'), (67, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 555 (char 554)'), (69, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 658 (char 657)'), (72, 'json decode error: Extra data: line 1 column 1171 (char 1170)'), (76, \"json decode error: Expecting ',' delimiter: line 1 column 979 (char 978)\"), (77, 'json decode error: Unterminated string starting at: line 17 column 5 (char 702)'), (78, \"json decode error: Expecting ',' delimiter: line 1 column 774 (char 773)\"), (79, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1177 (char 1176)'), (80, 'json decode error: Unterminated string starting at: line 1 column 2646 (char 2645)'), (83, 'json decode error: Extra data: line 44 column 2 (char 1362)'), (84, 'json decode error: Unterminated string starting at: line 94 column 15 (char 2233)'), (88, 'json decode error: Extra data: line 63 column 2 (char 1612)'), (90, 'json decode error: Unterminated string starting at: line 1 column 2670 (char 2669)'), (91, 'json decode error: Unterminated string starting at: line 1 column 2670 (char 2669)'), (92, 'json decode error: Unterminated string starting at: line 1 column 2670 (char 2669)'), (93, \"json decode error: Expecting ',' delimiter: line 1 column 1467 (char 1466)\"), (95, 'json decode error: Expecting value: line 1 column 1 (char 0)'), (96, 'json decode error: Expecting property name enclosed in double quotes: line 82 column 6 (char 2322)'), (97, \"json decode error: Expecting ',' delimiter: line 39 column 1 (char 1195)\"), (98, 'json decode error: Unterminated string starting at: line 56 column 19 (char 2120)'), (100, \"json decode error: Expecting ',' delimiter: line 1 column 802 (char 801)\"), (102, 'json decode error: Unterminated string starting at: line 109 column 11 (char 3089)'), (103, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (104, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (105, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (106, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (107, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (108, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (109, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (110, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (111, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (112, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (113, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (114, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (115, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (116, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (117, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (118, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (119, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (120, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (121, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (122, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (123, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (124, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (125, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (126, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (127, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (128, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (129, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (130, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (131, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (132, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (133, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (134, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (135, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (136, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (137, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (138, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (139, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (140, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (141, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (142, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (143, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (144, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (145, 'json decode error: Expecting value: line 1 column 1 (char 0)'), (146, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (147, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (148, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (149, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (150, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (151, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (152, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (153, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (154, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (155, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (156, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (157, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (158, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (159, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (160, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (161, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (162, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (163, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (164, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (165, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (166, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (167, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (168, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (169, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (170, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (171, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (172, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (173, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (174, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (180, 'json decode error: Extra data: line 1 column 1254 (char 1253)'), (182, 'json decode error: Unterminated string starting at: line 102 column 15 (char 2686)'), (185, 'json decode error: Extra data: line 1 column 767 (char 766)'), (186, 'json decode error: Extra data: line 39 column 2 (char 920)'), (188, 'json decode error: Extra data: line 1 column 966 (char 965)'), (195, 'json decode error: Invalid \\\\escape: line 1 column 652 (char 651)'), (197, \"json decode error: Expecting ',' delimiter: line 60 column 3 (char 1771)\"), (198, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (200, 'json decode error: Expecting value: line 1 column 2461 (char 2460)'), (201, 'json decode error: Unterminated string starting at: line 109 column 7 (char 2385)'), (202, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 236 (char 235)'), (205, 'json decode error: Unterminated string starting at: line 1 column 2616 (char 2615)'), (206, 'json decode error: Extra data: line 1 column 1103 (char 1102)'), (208, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (209, 'json decode error: Extra data: line 1 column 1649 (char 1648)'), (212, 'json decode error: Extra data: line 1 column 845 (char 844)'), (213, 'json decode error: Unterminated string starting at: line 1 column 2486 (char 2485)'), (215, 'json decode error: Extra data: line 1 column 641 (char 640)'), (216, 'json decode error: Unterminated string starting at: line 1 column 835 (char 834)'), (218, 'json decode error: Unterminated string starting at: line 96 column 20 (char 2835)'), (220, 'json decode error: Unterminated string starting at: line 1 column 2431 (char 2430)'), (225, 'json decode error: Extra data: line 1 column 1314 (char 1313)'), (226, 'json decode error: Unterminated string starting at: line 1 column 2040 (char 2039)'), (227, \"json decode error: Expecting ',' delimiter: line 67 column 1 (char 2346)\"), (228, 'json decode error: Unterminated string starting at: line 38 column 15 (char 3057)'), (229, 'json decode error: Extra data: line 1 column 1795 (char 1794)'), (230, \"json decode error: Expecting ':' delimiter: line 1 column 304 (char 303)\"), (231, 'json decode error: Unterminated string starting at: line 73 column 15 (char 3234)'), (234, \"json decode error: Expecting ',' delimiter: line 98 column 24 (char 2257)\"), (236, 'json decode error: Extra data: line 1 column 981 (char 980)'), (237, 'json decode error: Extra data: line 1 column 600 (char 599)'), (239, 'json decode error: Extra data: line 1 column 691 (char 690)'), (240, 'json decode error: Extra data: line 1 column 1049 (char 1048)'), (242, 'json decode error: Expecting value: line 1 column 2472 (char 2471)'), (249, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 462 (char 461)'), (251, 'json decode error: Extra data: line 1 column 1182 (char 1181)'), (252, 'json decode error: Extra data: line 1 column 1572 (char 1571)'), (256, \"json decode error: Expecting ',' delimiter: line 1 column 896 (char 895)\"), (257, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 839 (char 838)'), (259, \"json decode error: Expecting ',' delimiter: line 128 column 46 (char 3080)\"), (260, 'json decode error: Extra data: line 62 column 2 (char 1618)'), (263, 'json decode error: Unterminated string starting at: line 1 column 2436 (char 2435)'), (264, 'json decode error: Unterminated string starting at: line 151 column 20 (char 3191)'), (265, 'json decode error: Expecting property name enclosed in double quotes: line 148 column 29 (char 3200)'), (269, 'json decode error: Extra data: line 1 column 547 (char 546)'), (270, 'json decode error: Unterminated string starting at: line 1 column 2373 (char 2372)'), (274, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 583 (char 582)'), (275, 'json decode error: Expecting value: line 138 column 14 (char 2282)'), (276, 'json decode error: Extra data: line 1 column 674 (char 673)'), (278, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 849 (char 848)'), (279, 'json decode error: Extra data: line 1 column 913 (char 912)'), (280, 'json decode error: Extra data: line 1 column 954 (char 953)'), (282, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1724 (char 1723)'), (283, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1055 (char 1054)'), (284, 'json decode error: Expecting value: line 125 column 15 (char 2613)'), (288, 'json decode error: Unterminated string starting at: line 127 column 14 (char 2819)'), (289, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 524 (char 523)'), (292, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 608 (char 607)'), (293, 'json decode error: Extra data: line 1 column 1608 (char 1607)'), (294, 'json decode error: Extra data: line 1 column 799 (char 798)'), (298, 'json decode error: Unterminated string starting at: line 14 column 15 (char 427)'), (300, 'json decode error: Extra data: line 56 column 1 (char 2361)'), (304, 'json decode error: Extra data: line 1 column 894 (char 893)'), (307, 'json decode error: Extra data: line 47 column 2 (char 1569)'), (309, 'json decode error: Extra data: line 1 column 741 (char 740)'), (311, 'json decode error: Unterminated string starting at: line 1 column 2349 (char 2348)'), (313, 'json decode error: Unterminated string starting at: line 1 column 2076 (char 2075)'), (314, 'json decode error: Extra data: line 1 column 1002 (char 1001)'), (315, 'json decode error: Extra data: line 42 column 2 (char 1068)'), (316, 'json decode error: Extra data: line 1 column 1170 (char 1169)'), (317, 'json decode error: Expecting value: line 165 column 11 (char 3336)'), (318, 'json decode error: Extra data: line 1 column 932 (char 931)'), (320, 'json decode error: Extra data: line 1 column 1220 (char 1219)'), (321, 'json decode error: Extra data: line 1 column 947 (char 946)'), (324, 'json decode error: Extra data: line 1 column 1412 (char 1411)'), (325, 'json decode error: Unterminated string starting at: line 1 column 2345 (char 2344)'), (328, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2111 (char 2110)'), (329, 'json decode error: Extra data: line 1 column 964 (char 963)'), (330, 'json decode error: Extra data: line 1 column 919 (char 918)'), (332, \"json decode error: Expecting ',' delimiter: line 1 column 274 (char 273)\"), (333, 'json decode error: Extra data: line 1 column 1137 (char 1136)'), (334, 'json decode error: Invalid \\\\uXXXX escape: line 1 column 1359 (char 1358)'), (336, 'json decode error: Unterminated string starting at: line 153 column 20 (char 3017)'), (337, \"json decode error: Expecting ',' delimiter: line 1 column 2368 (char 2367)\"), (338, 'json decode error: Unterminated string starting at: line 1 column 2234 (char 2233)'), (339, 'json decode error: Invalid \\\\uXXXX escape: line 6 column 1112 (char 1196)'), (341, \"json decode error: Expecting ',' delimiter: line 1 column 997 (char 996)\"), (342, \"json decode error: Expecting ',' delimiter: line 1 column 258 (char 257)\"), (346, 'json decode error: Extra data: line 1 column 1106 (char 1105)'), (347, 'json decode error: Extra data: line 1 column 757 (char 756)'), (348, 'json decode error: Unterminated string starting at: line 1 column 2219 (char 2218)'), (349, 'json decode error: Unterminated string starting at: line 1 column 2162 (char 2161)'), (350, 'json decode error: Unterminated string starting at: line 88 column 11 (char 1903)'), (351, 'json decode error: Expecting value: line 1 column 2172 (char 2171)'), (352, \"json decode error: Expecting ',' delimiter: line 1 column 1583 (char 1582)\"), (358, \"json decode error: Expecting ',' delimiter: line 1 column 1050 (char 1049)\"), (360, 'json decode error: Extra data: line 94 column 2 (char 2556)'), (361, 'json decode error: Unterminated string starting at: line 111 column 14 (char 2582)'), (366, 'json decode error: Extra data: line 1 column 1263 (char 1262)'), (367, 'json decode error: Unterminated string starting at: line 1 column 472 (char 471)'), (370, 'json decode error: Unterminated string starting at: line 1 column 2414 (char 2413)'), (371, 'json decode error: Expecting value: line 1 column 2193 (char 2192)'), (372, \"json decode error: Expecting ',' delimiter: line 126 column 123 (char 2493)\"), (373, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 129 (char 128)'), (374, 'json decode error: Unterminated string starting at: line 1 column 1871 (char 1870)'), (379, 'json decode error: Extra data: line 39 column 2 (char 955)'), (381, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (382, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (384, 'json decode error: Extra data: line 65 column 2 (char 1805)'), (385, 'json decode error: Extra data: line 1 column 1689 (char 1688)'), (386, 'json decode error: Extra data: line 1 column 1375 (char 1374)'), (388, 'json decode error: Unterminated string starting at: line 1 column 2298 (char 2297)'), (389, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 251 (char 250)'), (392, 'json decode error: Extra data: line 1 column 1468 (char 1467)'), (393, \"json decode error: Expecting ',' delimiter: line 1 column 165 (char 164)\"), (394, 'json decode error: Extra data: line 37 column 2 (char 898)'), (395, \"json decode error: Expecting ',' delimiter: line 1 column 509 (char 508)\"), (396, 'json decode error: Extra data: line 1 column 1326 (char 1325)'), (397, 'json decode error: Extra data: line 1 column 986 (char 985)'), (400, 'json decode error: Extra data: line 1 column 1104 (char 1103)'), (402, 'json decode error: Invalid \\\\uXXXX escape: line 1 column 1541 (char 1540)'), (405, 'json decode error: Invalid \\\\uXXXX escape: line 1 column 1480 (char 1479)'), (406, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 941 (char 940)'), (407, 'json decode error: Extra data: line 1 column 1404 (char 1403)'), (408, 'json decode error: Unterminated string starting at: line 1 column 2053 (char 2052)'), (410, 'json decode error: Extra data: line 1 column 1360 (char 1359)'), (411, 'json decode error: Extra data: line 1 column 1125 (char 1124)'), (413, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 827 (char 826)'), (414, 'json decode error: Extra data: line 1 column 1764 (char 1763)'), (415, 'json decode error: Extra data: line 33 column 2 (char 868)'), (416, 'json decode error: Extra data: line 1 column 1532 (char 1531)'), (418, 'json decode error: Extra data: line 33 column 2 (char 880)'), (419, 'json decode error: Unterminated string starting at: line 1 column 2455 (char 2454)'), (420, 'json decode error: Unterminated string starting at: line 1 column 2394 (char 2393)'), (421, 'json decode error: Extra data: line 1 column 1892 (char 1891)'), (422, 'json decode error: Extra data: line 1 column 1795 (char 1794)'), (423, 'json decode error: Extra data: line 1 column 989 (char 988)'), (424, \"json decode error: Expecting ',' delimiter: line 1 column 890 (char 889)\"), (427, 'json decode error: Extra data: line 1 column 1367 (char 1366)'), (429, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 931 (char 930)'), (430, 'json decode error: Extra data: line 1 column 1275 (char 1274)'), (433, 'json decode error: Extra data: line 1 column 1423 (char 1422)'), (435, 'json decode error: Extra data: line 1 column 833 (char 832)'), (436, 'json decode error: Extra data: line 1 column 1075 (char 1074)'), (437, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 127 (char 126)'), (438, 'json decode error: Extra data: line 1 column 899 (char 898)'), (442, \"json decode error: Expecting ',' delimiter: line 1 column 350 (char 349)\"), (444, 'json decode error: Extra data: line 1 column 1766 (char 1765)'), (446, 'json decode error: Extra data: line 1 column 500 (char 499)'), (447, 'json decode error: Extra data: line 1 column 1275 (char 1274)'), (448, \"json decode error: Expecting ',' delimiter: line 40 column 11 (char 1357)\"), (449, 'json decode error: Extra data: line 1 column 2013 (char 2012)'), (452, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 153 (char 152)'), (455, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1049 (char 1048)'), (456, 'json decode error: Unterminated string starting at: line 1 column 393 (char 392)'), (457, 'json decode error: Extra data: line 1 column 1194 (char 1193)'), (462, 'json decode error: Extra data: line 1 column 1673 (char 1672)'), (463, 'no separator')]\n",
      "🔄 Reprocessing page 003 (prev failure: json decode error: Extra data: line 1 column 700 (char 699))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m ocr_text \u001b[38;5;241m=\u001b[39m ocr_image(img)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 3. regenerate caption\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m caption \u001b[38;5;241m=\u001b[39m \u001b[43mcaption_with_phi4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# 4. write raw caption\u001b[39;00m\n\u001b[1;32m     87\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 64\u001b[0m, in \u001b[0;36mcaption_with_phi4\u001b[0;34m(img, system, user)\u001b[0m\n\u001b[1;32m     62\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimg, text\u001b[38;5;241m=\u001b[39mfull_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 64\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processor\u001b[38;5;241m.\u001b[39mdecode(out[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:2116\u001b[0m, in \u001b[0;36mPhi4MMForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, input_mode, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input_mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_image_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_audio_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_audio_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_projection_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_projection_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2135\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1755\u001b[0m, in \u001b[0;36mPhi4MMModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, audio_projection_mode, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1745\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1746\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1752\u001b[0m         cache_position,\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1755\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1459\u001b[0m, in \u001b[0;36mPhi4MMDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m-> 1459\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(attn_outputs)\n\u001b[1;32m   1471\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1211\u001b[0m, in \u001b[0;36mPhi4MMFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m   1207\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m-> 1211\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m query_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m   1213\u001b[0m query_states \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :query_pos]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:727\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_input_dtype(x, lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m scaling\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dropout, nn\u001b[38;5;241m.\u001b[39mIdentity) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, gc, json\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- your constants & model setup ---\n",
    "FILE = \"AI.pdf\"\n",
    "output_dir = \"pdf_pages\"\n",
    "# 你的 Slide 分析 prompt，當成 system 角色\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI lecture slide analyzer. The following input is an image of a lecture slide about “Artificial Intelligence.” \n",
    "1. Extract every piece of written content:\n",
    "   • Slide title\n",
    "   • Section or bullet headings\n",
    "   • Sub-bullets and their full text\n",
    "   • Definitions, formulas, and any inline examples\n",
    "2. Describe every visual element:\n",
    "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
    "   • Icons or figures (what they represent and any attached caption)\n",
    "3. Organize your output as a JSON object with these fields:\n",
    "   {\n",
    "     \"title\": string,\n",
    "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
    "     \"definitions\": { term: definition, … },\n",
    "     \"formulas\": [ string, … ],\n",
    "     \"examples\": [ string, … ],\n",
    "     \"diagrams\": [\n",
    "       {\n",
    "         \"type\": string,\n",
    "         \"elements\": [\n",
    "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
    "         ],\n",
    "         \"relationships\": [ string, … ]\n",
    "       }, …\n",
    "     ]\n",
    "   }\n",
    "4. Finally, produce a brief “summary” string of the slide's core message.\n",
    "Be as thorough and precise as possible—this will be used for later retrieval and generation.\n",
    "5. When you reply, output *only* the JSON object—no extra words\n",
    "\"\"\"\n",
    "model_id = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "generation_config.max_new_tokens = 1024\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, device_map=\"cuda\", torch_dtype=\"auto\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "def ocr_image(img: Image.Image) -> str:\n",
    "    return pytesseract.image_to_string(img, lang=\"chi_tra+eng\")\n",
    "\n",
    "def caption_with_phi4(img: Image.Image, system: str, user: str) -> str:\n",
    "    full_prompt = (\n",
    "        \"<|im_start|>system<|im_sep|>\" + system.strip() + \"<|im_end|>\"\n",
    "        \"<|im_start|>user<|im_sep|>\" + user.strip() + \"<|image_1|><|im_end|>\"\n",
    "        \"<|im_start|>assistant<|im_sep|>\"\n",
    "    )\n",
    "    inputs = processor(images=img, text=full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, generation_config=generation_config)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "reader = PdfReader(FILE)\n",
    "sep = \"<|im_sep|>\"\n",
    "print(len(failed_pages))\n",
    "print(failed_pages)\n",
    "\n",
    "for page_num, reason in failed_pages:\n",
    "    print(f\"🔄 Reprocessing page {page_num:03d} (prev failure: {reason})\")\n",
    "    # 1. render page → image\n",
    "    images = convert_from_path(FILE, dpi=200,\n",
    "                               first_page=page_num, last_page=page_num,\n",
    "                               use_pdftocairo=True)\n",
    "    img = images[0]\n",
    "\n",
    "    # 2. OCR\n",
    "    ocr_text = ocr_image(img)\n",
    "\n",
    "    # 3. regenerate caption\n",
    "    caption = caption_with_phi4(img, SYSTEM_PROMPT, ocr_text)\n",
    "\n",
    "    # 4. write raw caption\n",
    "    base = f\"page_{page_num:03d}\"\n",
    "    with open(os.path.join(output_dir, base + \"_caption.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(caption)\n",
    "\n",
    "    # 5. strip out JSON\n",
    "    idx = caption.rfind(sep)\n",
    "    if idx == -1:\n",
    "        print(f\"⚠️ Still no separator on page {page_num}\")\n",
    "    else:\n",
    "        json_str = caption[idx + len(sep):].strip()\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            with open(os.path.join(output_dir, base + \"_caption_strip.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"✅  page_{page_num:03d}_caption_strip.json written\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌  JSON still invalid on page {page_num}: {e}\")\n",
    "\n",
    "    # cleanup\n",
    "    del img, images\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02234344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed cases: [(3, 'json decode error: Extra data: line 1 column 700 (char 699)'), (5, 'json decode error: Expecting value: line 1 column 1 (char 0)'), (6, 'json decode error: Unterminated string starting at: line 1 column 2323 (char 2322)'), (10, 'json decode error: Extra data: line 1 column 1318 (char 1317)'), (13, 'json decode error: Extra data: line 1 column 745 (char 744)'), (14, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (15, 'json decode error: Unterminated string starting at: line 1 column 2693 (char 2692)'), (18, 'json decode error: Extra data: line 1 column 1120 (char 1119)'), (20, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 174 (char 173)'), (21, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 174 (char 173)'), (23, 'json decode error: Invalid control character at: line 1 column 64 (char 63)'), (24, 'json decode error: Extra data: line 1 column 961 (char 960)'), (27, 'json decode error: Extra data: line 1 column 2050 (char 2049)'), (28, 'json decode error: Unterminated string starting at: line 1 column 2543 (char 2542)'), (29, 'json decode error: Unterminated string starting at: line 127 column 20 (char 2965)'), (31, 'json decode error: Extra data: line 1 column 702 (char 701)'), (34, \"json decode error: Expecting ',' delimiter: line 116 column 3 (char 2635)\"), (38, 'json decode error: Extra data: line 1 column 1333 (char 1332)'), (39, 'json decode error: Extra data: line 1 column 1352 (char 1351)'), (41, 'json decode error: Extra data: line 1 column 1058 (char 1057)'), (47, \"json decode error: Expecting ',' delimiter: line 1 column 945 (char 944)\"), (52, 'json decode error: Extra data: line 1 column 2085 (char 2084)'), (55, 'json decode error: Extra data: line 1 column 1389 (char 1388)'), (57, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (62, 'json decode error: Extra data: line 1 column 852 (char 851)'), (67, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 555 (char 554)'), (69, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 658 (char 657)'), (72, 'json decode error: Extra data: line 1 column 1171 (char 1170)'), (76, \"json decode error: Expecting ',' delimiter: line 1 column 979 (char 978)\"), (77, 'json decode error: Unterminated string starting at: line 17 column 5 (char 702)'), (78, \"json decode error: Expecting ',' delimiter: line 1 column 774 (char 773)\"), (79, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1177 (char 1176)'), (80, 'json decode error: Unterminated string starting at: line 1 column 2646 (char 2645)'), (83, 'json decode error: Extra data: line 44 column 2 (char 1362)'), (84, 'json decode error: Unterminated string starting at: line 94 column 15 (char 2233)'), (88, 'json decode error: Extra data: line 63 column 2 (char 1612)'), (90, 'json decode error: Unterminated string starting at: line 1 column 2670 (char 2669)'), (91, 'json decode error: Unterminated string starting at: line 1 column 2670 (char 2669)'), (92, 'json decode error: Unterminated string starting at: line 1 column 2670 (char 2669)'), (93, \"json decode error: Expecting ',' delimiter: line 1 column 1467 (char 1466)\"), (95, 'json decode error: Expecting value: line 1 column 1 (char 0)'), (96, 'json decode error: Expecting property name enclosed in double quotes: line 82 column 6 (char 2322)'), (97, \"json decode error: Expecting ',' delimiter: line 39 column 1 (char 1195)\"), (98, 'json decode error: Unterminated string starting at: line 56 column 19 (char 2120)'), (100, \"json decode error: Expecting ',' delimiter: line 1 column 802 (char 801)\"), (102, 'json decode error: Unterminated string starting at: line 109 column 11 (char 3089)'), (103, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (104, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (105, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (106, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (107, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (108, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (109, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (110, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (111, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (112, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (113, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (114, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (115, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (116, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (117, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (118, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (119, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (120, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (121, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (122, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (123, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (124, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (125, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (126, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (127, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (128, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (129, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (130, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (131, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (132, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (133, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (134, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (135, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (136, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (137, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (138, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (139, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (140, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (141, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (142, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (143, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (144, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (145, 'json decode error: Expecting value: line 1 column 1 (char 0)'), (146, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (147, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (148, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (149, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (150, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (151, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (152, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (153, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (154, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (155, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (156, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (157, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (158, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (159, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (160, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (161, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (162, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (163, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (164, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (165, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (166, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (167, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (168, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (169, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (170, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (171, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (172, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (173, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (174, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (180, 'json decode error: Extra data: line 1 column 1254 (char 1253)'), (182, 'json decode error: Unterminated string starting at: line 102 column 15 (char 2686)'), (185, 'json decode error: Extra data: line 1 column 767 (char 766)'), (186, 'json decode error: Extra data: line 39 column 2 (char 920)'), (188, 'json decode error: Extra data: line 1 column 966 (char 965)'), (195, 'json decode error: Invalid \\\\escape: line 1 column 652 (char 651)'), (197, \"json decode error: Expecting ',' delimiter: line 60 column 3 (char 1771)\"), (198, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (200, 'json decode error: Expecting value: line 1 column 2461 (char 2460)'), (201, 'json decode error: Unterminated string starting at: line 109 column 7 (char 2385)'), (202, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 236 (char 235)'), (205, 'json decode error: Unterminated string starting at: line 1 column 2616 (char 2615)'), (206, 'json decode error: Extra data: line 1 column 1103 (char 1102)'), (208, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (209, 'json decode error: Extra data: line 1 column 1649 (char 1648)'), (212, 'json decode error: Extra data: line 1 column 845 (char 844)'), (213, 'json decode error: Unterminated string starting at: line 1 column 2486 (char 2485)'), (215, 'json decode error: Extra data: line 1 column 641 (char 640)'), (216, 'json decode error: Unterminated string starting at: line 1 column 835 (char 834)'), (218, 'json decode error: Unterminated string starting at: line 96 column 20 (char 2835)'), (220, 'json decode error: Unterminated string starting at: line 1 column 2431 (char 2430)'), (225, 'json decode error: Extra data: line 1 column 1314 (char 1313)'), (226, 'json decode error: Unterminated string starting at: line 1 column 2040 (char 2039)'), (227, \"json decode error: Expecting ',' delimiter: line 67 column 1 (char 2346)\"), (228, 'json decode error: Unterminated string starting at: line 38 column 15 (char 3057)'), (229, 'json decode error: Extra data: line 1 column 1795 (char 1794)'), (230, \"json decode error: Expecting ':' delimiter: line 1 column 304 (char 303)\"), (231, 'json decode error: Unterminated string starting at: line 73 column 15 (char 3234)'), (234, \"json decode error: Expecting ',' delimiter: line 98 column 24 (char 2257)\"), (236, 'json decode error: Extra data: line 1 column 981 (char 980)'), (237, 'json decode error: Extra data: line 1 column 600 (char 599)'), (239, 'json decode error: Extra data: line 1 column 691 (char 690)'), (240, 'json decode error: Extra data: line 1 column 1049 (char 1048)'), (242, 'json decode error: Expecting value: line 1 column 2472 (char 2471)'), (249, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 462 (char 461)'), (251, 'json decode error: Extra data: line 1 column 1182 (char 1181)'), (252, 'json decode error: Extra data: line 1 column 1572 (char 1571)'), (256, \"json decode error: Expecting ',' delimiter: line 1 column 896 (char 895)\"), (257, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 839 (char 838)'), (259, \"json decode error: Expecting ',' delimiter: line 128 column 46 (char 3080)\"), (260, 'json decode error: Extra data: line 62 column 2 (char 1618)'), (263, 'json decode error: Unterminated string starting at: line 1 column 2436 (char 2435)'), (264, 'json decode error: Unterminated string starting at: line 151 column 20 (char 3191)'), (265, 'json decode error: Expecting property name enclosed in double quotes: line 148 column 29 (char 3200)'), (269, 'json decode error: Extra data: line 1 column 547 (char 546)'), (270, 'json decode error: Unterminated string starting at: line 1 column 2373 (char 2372)'), (274, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 583 (char 582)'), (275, 'json decode error: Expecting value: line 138 column 14 (char 2282)'), (276, 'json decode error: Extra data: line 1 column 674 (char 673)'), (278, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 849 (char 848)'), (279, 'json decode error: Extra data: line 1 column 913 (char 912)'), (280, 'json decode error: Extra data: line 1 column 954 (char 953)'), (282, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1724 (char 1723)'), (283, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1055 (char 1054)'), (284, 'json decode error: Expecting value: line 125 column 15 (char 2613)'), (288, 'json decode error: Unterminated string starting at: line 127 column 14 (char 2819)'), (289, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 524 (char 523)'), (292, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 608 (char 607)'), (293, 'json decode error: Extra data: line 1 column 1608 (char 1607)'), (294, 'json decode error: Extra data: line 1 column 799 (char 798)'), (298, 'json decode error: Unterminated string starting at: line 14 column 15 (char 427)'), (300, 'json decode error: Extra data: line 56 column 1 (char 2361)'), (304, 'json decode error: Extra data: line 1 column 894 (char 893)'), (307, 'json decode error: Extra data: line 47 column 2 (char 1569)'), (309, 'json decode error: Extra data: line 1 column 741 (char 740)'), (311, 'json decode error: Unterminated string starting at: line 1 column 2349 (char 2348)'), (313, 'json decode error: Unterminated string starting at: line 1 column 2076 (char 2075)'), (314, 'json decode error: Extra data: line 1 column 1002 (char 1001)'), (315, 'json decode error: Extra data: line 42 column 2 (char 1068)'), (316, 'json decode error: Extra data: line 1 column 1170 (char 1169)'), (317, 'json decode error: Expecting value: line 165 column 11 (char 3336)'), (318, 'json decode error: Extra data: line 1 column 932 (char 931)'), (320, 'json decode error: Extra data: line 1 column 1220 (char 1219)'), (321, 'json decode error: Extra data: line 1 column 947 (char 946)'), (324, 'json decode error: Extra data: line 1 column 1412 (char 1411)'), (325, 'json decode error: Unterminated string starting at: line 1 column 2345 (char 2344)'), (328, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2111 (char 2110)'), (329, 'json decode error: Extra data: line 1 column 964 (char 963)'), (330, 'json decode error: Extra data: line 1 column 919 (char 918)'), (332, \"json decode error: Expecting ',' delimiter: line 1 column 274 (char 273)\"), (333, 'json decode error: Extra data: line 1 column 1137 (char 1136)'), (334, 'json decode error: Invalid \\\\uXXXX escape: line 1 column 1359 (char 1358)'), (336, 'json decode error: Unterminated string starting at: line 153 column 20 (char 3017)'), (337, \"json decode error: Expecting ',' delimiter: line 1 column 2368 (char 2367)\"), (338, 'json decode error: Unterminated string starting at: line 1 column 2234 (char 2233)'), (339, 'json decode error: Invalid \\\\uXXXX escape: line 6 column 1112 (char 1196)'), (341, \"json decode error: Expecting ',' delimiter: line 1 column 997 (char 996)\"), (342, \"json decode error: Expecting ',' delimiter: line 1 column 258 (char 257)\"), (346, 'json decode error: Extra data: line 1 column 1106 (char 1105)'), (347, 'json decode error: Extra data: line 1 column 757 (char 756)'), (348, 'json decode error: Unterminated string starting at: line 1 column 2219 (char 2218)'), (349, 'json decode error: Unterminated string starting at: line 1 column 2162 (char 2161)'), (350, 'json decode error: Unterminated string starting at: line 88 column 11 (char 1903)'), (351, 'json decode error: Expecting value: line 1 column 2172 (char 2171)'), (352, \"json decode error: Expecting ',' delimiter: line 1 column 1583 (char 1582)\"), (358, \"json decode error: Expecting ',' delimiter: line 1 column 1050 (char 1049)\"), (360, 'json decode error: Extra data: line 94 column 2 (char 2556)'), (361, 'json decode error: Unterminated string starting at: line 111 column 14 (char 2582)'), (366, 'json decode error: Extra data: line 1 column 1263 (char 1262)'), (367, 'json decode error: Unterminated string starting at: line 1 column 472 (char 471)'), (370, 'json decode error: Unterminated string starting at: line 1 column 2414 (char 2413)'), (371, 'json decode error: Expecting value: line 1 column 2193 (char 2192)'), (372, \"json decode error: Expecting ',' delimiter: line 126 column 123 (char 2493)\"), (373, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 129 (char 128)'), (374, 'json decode error: Unterminated string starting at: line 1 column 1871 (char 1870)'), (379, 'json decode error: Extra data: line 39 column 2 (char 955)'), (381, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (382, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)'), (384, 'json decode error: Extra data: line 65 column 2 (char 1805)'), (385, 'json decode error: Extra data: line 1 column 1689 (char 1688)'), (386, 'json decode error: Extra data: line 1 column 1375 (char 1374)'), (388, 'json decode error: Unterminated string starting at: line 1 column 2298 (char 2297)'), (389, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 251 (char 250)'), (392, 'json decode error: Extra data: line 1 column 1468 (char 1467)'), (393, \"json decode error: Expecting ',' delimiter: line 1 column 165 (char 164)\"), (394, 'json decode error: Extra data: line 37 column 2 (char 898)'), (395, \"json decode error: Expecting ',' delimiter: line 1 column 509 (char 508)\"), (396, 'json decode error: Extra data: line 1 column 1326 (char 1325)'), (397, 'json decode error: Extra data: line 1 column 986 (char 985)'), (400, 'json decode error: Extra data: line 1 column 1104 (char 1103)'), (402, 'json decode error: Invalid \\\\uXXXX escape: line 1 column 1541 (char 1540)'), (405, 'json decode error: Invalid \\\\uXXXX escape: line 1 column 1480 (char 1479)'), (406, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 941 (char 940)'), (407, 'json decode error: Extra data: line 1 column 1404 (char 1403)'), (408, 'json decode error: Unterminated string starting at: line 1 column 2053 (char 2052)'), (410, 'json decode error: Extra data: line 1 column 1360 (char 1359)'), (411, 'json decode error: Extra data: line 1 column 1125 (char 1124)'), (413, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 827 (char 826)'), (414, 'json decode error: Extra data: line 1 column 1764 (char 1763)'), (415, 'json decode error: Extra data: line 33 column 2 (char 868)'), (416, 'json decode error: Extra data: line 1 column 1532 (char 1531)'), (418, 'json decode error: Extra data: line 33 column 2 (char 880)'), (419, 'json decode error: Unterminated string starting at: line 1 column 2455 (char 2454)'), (420, 'json decode error: Unterminated string starting at: line 1 column 2394 (char 2393)'), (421, 'json decode error: Extra data: line 1 column 1892 (char 1891)'), (422, 'json decode error: Extra data: line 1 column 1795 (char 1794)'), (423, 'json decode error: Extra data: line 1 column 989 (char 988)'), (424, \"json decode error: Expecting ',' delimiter: line 1 column 890 (char 889)\"), (427, 'json decode error: Extra data: line 1 column 1367 (char 1366)'), (429, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 931 (char 930)'), (430, 'json decode error: Extra data: line 1 column 1275 (char 1274)'), (433, 'json decode error: Extra data: line 1 column 1423 (char 1422)'), (435, 'json decode error: Extra data: line 1 column 833 (char 832)'), (436, 'json decode error: Extra data: line 1 column 1075 (char 1074)'), (437, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 127 (char 126)'), (438, 'json decode error: Extra data: line 1 column 899 (char 898)'), (442, \"json decode error: Expecting ',' delimiter: line 1 column 350 (char 349)\"), (444, 'json decode error: Extra data: line 1 column 1766 (char 1765)'), (446, 'json decode error: Extra data: line 1 column 500 (char 499)'), (447, 'json decode error: Extra data: line 1 column 1275 (char 1274)'), (448, \"json decode error: Expecting ',' delimiter: line 40 column 11 (char 1357)\"), (449, 'json decode error: Extra data: line 1 column 2013 (char 2012)'), (452, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 153 (char 152)'), (455, 'json decode error: Expecting property name enclosed in double quotes: line 1 column 1049 (char 1048)'), (456, 'json decode error: Unterminated string starting at: line 1 column 393 (char 392)'), (457, 'json decode error: Extra data: line 1 column 1194 (char 1193)'), (462, 'json decode error: Extra data: line 1 column 1673 (char 1672)'), (463, 'no separator')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "output_dir = \"pdf_pages\"\n",
    "sep = \"<|im_sep|>\"\n",
    "\n",
    "failed_pages = []\n",
    "\n",
    "for page_num in range(1, 464):\n",
    "    in_path = os.path.join(output_dir, f\"page_{page_num:03d}_caption.json\")\n",
    "    out_path = os.path.join(output_dir, f\"page_{page_num:03d}_caption_strip.json\")\n",
    "\n",
    "    # read the raw caption\n",
    "    try:\n",
    "        with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw = f.read()\n",
    "    except FileNotFoundError:\n",
    "        failed_pages.append((page_num, \"missing file\"))\n",
    "        continue\n",
    "\n",
    "    # find the last separator\n",
    "    idx = raw.rfind(sep)\n",
    "    if idx == -1:\n",
    "        failed_pages.append((page_num, \"no separator\"))\n",
    "        continue\n",
    "\n",
    "    # extract the JSON payload\n",
    "    json_str = raw[idx + len(sep):].strip()\n",
    "\n",
    "    # parse and validate\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        failed_pages.append((page_num, f\"json decode error: {e}\"))\n",
    "        continue\n",
    "\n",
    "    # write the stripped JSON\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# After processing:\n",
    "print(\"Failed cases:\", failed_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49030ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from langchain.schema import Document\n",
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b991816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths and constants\n",
    "FILE = \"AI.pdf\"  # Path to the 463-page PDF\n",
    "DB_PATH = \"./chroma_db_task2\"  # Path to store Chroma database\n",
    "EMBEDDINGS = \"all-MiniLM-L6-v2\"  # Embedding model\n",
    "QUERY = \"On which page can you find a comparison of two dynamic programming methods for solving Markov Decision Processes (MDPs), focusing on how iterative reward estimation and iterative strategy optimization compute all optimal values while differing in their update processes and policy handling?\"\n",
    "QUERY_ID = \"test_001\"  # Dummy ID for the test case\n",
    "output_dir = \"pdf_pages\"\n",
    "\n",
    "# 你的 Slide 分析 prompt，當成 system 角色\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI lecture slide analyzer. The following input is an image of a lecture slide about “Artificial Intelligence.” \n",
    "1. Extract every piece of written content:\n",
    "   • Slide title\n",
    "   • Section or bullet headings\n",
    "   • Sub-bullets and their full text\n",
    "   • Definitions, formulas, and any inline examples\n",
    "2. Describe every visual element:\n",
    "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
    "   • Icons or figures (what they represent and any attached caption)\n",
    "3. Organize your output as a JSON object with these fields:\n",
    "   {\n",
    "     \"title\": string,\n",
    "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
    "     \"definitions\": { term: definition, … },\n",
    "     \"formulas\": [ string, … ],\n",
    "     \"examples\": [ string, … ],\n",
    "     \"diagrams\": [\n",
    "       {\n",
    "         \"type\": string,\n",
    "         \"elements\": [\n",
    "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
    "         ],\n",
    "         \"relationships\": [ string, … ]\n",
    "       }, …\n",
    "     ]\n",
    "   }\n",
    "4. Finally, produce a brief “summary” string of the slide's core message.\n",
    "Be as thorough and precise as possible—this will be used for later retrieval and generation.\n",
    "5. When you reply, output *only* the JSON object—no extra words\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c4fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "\n",
    "# # 用新參數方式，別再用 Settings\n",
    "# client = chromadb.PersistentClient(\n",
    "#     path=DB_PATH\n",
    "# )\n",
    "\n",
    "# # 取出你原本的 collection\n",
    "# col = client.get_collection(\"langchain\")\n",
    "\n",
    "# # 從 offset=0 開始，分批讀所有 documents\n",
    "# batch_size = 100\n",
    "# offset = 0\n",
    "# while True:\n",
    "#     res = col.get(\n",
    "#         include=[\"documents\",\"metadatas\"],\n",
    "#         limit=batch_size,\n",
    "#         offset=offset\n",
    "#     )\n",
    "#     docs = res[\"documents\"]\n",
    "#     metas = res[\"metadatas\"]\n",
    "#     if not docs:\n",
    "#         break\n",
    "\n",
    "#     for d, m in zip(docs, metas):\n",
    "#         print(f'page={m[\"page\"]} type={m[\"type\"]} snippet={d[:50]!r}')\n",
    "\n",
    "#     offset += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5577bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 150 0 (offset 0)\n",
      "Ignoring wrong pointing object 168 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Ignoring wrong pointing object 241 0 (offset 0)\n",
      "Ignoring wrong pointing object 251 0 (offset 0)\n",
      "Ignoring wrong pointing object 257 0 (offset 0)\n",
      "Ignoring wrong pointing object 281 0 (offset 0)\n",
      "Ignoring wrong pointing object 289 0 (offset 0)\n",
      "Ignoring wrong pointing object 294 0 (offset 0)\n",
      "Ignoring wrong pointing object 336 0 (offset 0)\n",
      "Ignoring wrong pointing object 346 0 (offset 0)\n",
      "Ignoring wrong pointing object 370 0 (offset 0)\n",
      "Ignoring wrong pointing object 417 0 (offset 0)\n",
      "Ignoring wrong pointing object 419 0 (offset 0)\n",
      "Ignoring wrong pointing object 445 0 (offset 0)\n",
      "Ignoring wrong pointing object 457 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# # —— 先載你原本的 PDF loader & splitter，得到 docs ——  \n",
    "# loader = PyPDFLoader(FILE)\n",
    "# pages = loader.load_and_split()  \n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# docs = splitter.split_documents(pages)\n",
    "\n",
    "# # save into chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5a299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n",
      "/home/labstudent/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PyPDF2 import PdfReader\n",
    "import torch\n",
    "\n",
    "# Set generation config\n",
    "model_id = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.num_logits_to_keep = 100\n",
    "do_sample=True\n",
    "temperature=0.1\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    _attn_implementation=\"flash_attention_2\"\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90401e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 1] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slidePreliminaries and Al Agent Basics\n",
      "\n",
      "7e, Wen-Huang Cheng (BBX =)\n",
      "\n",
      "National Taiwan University\n",
      "\n",
      " \n",
      "\n",
      "wenhuang@csie.ntu.edu.tw<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"Artificial Intelligence\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Preliminaries and AI Agent Basics\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"icon\",\n",
      "      \"elements\": [\n",
      "        {\n",
      "          \"shape\": \"head\",\n",
      "          \"label\": \"AI\",\n",
      "          \"notes\": \"Represents artificial intelligence\"\n",
      "        }\n",
      "      ],\n",
      "      \"relationships\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "\"Summary\": \"The slide is an introduction to Artificial Intelligence, focusing on preliminaries and AI agent basics. It features a title, a section heading, and an icon representing AI. The presenter's name, affiliation, and contact information are also provided.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 2] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideNe <\n",
      "\n",
      "SkillMimic: Learning Reusable Basketball Skills from Demonstrations\n",
      "\n",
      "a eV\n",
      "\n",
      " \n",
      "\n",
      "Yinhuai Wang*, Qihan Zhao*, Runyi Yu“, Ailing Zeng, Jing lin, Zhengyi Luo, Hok Wai Tsui, Jiwen Yu,\n",
      "Xiu Li, Qifeng Chen, Jian Zhang, Lei Zhang, Ping Tan\n",
      "\n",
      "  \n",
      "\n",
      "© FEARKS                                    Sm      >          SE -            °                                      多       Carnegie\n",
      "(WD HE HONG Kone universiry oF                   sm GS ween? it       % idea=                          f        Warnes\n",
      "SCIENCE AND TECHNOLOGY         unl TREE        cay  de nK a Wy   AFA2     Ided =\"    Te ncenr      WY      = aol ty<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"SkillMimic: Learning Reusable Basketball Skills from Demonstrations\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"SkillMimic: Learning Reusable Basketball Skills from Demonstrations\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"basketball court\",\n",
      "      \"elements\": [\n",
      "        {\n",
      "          \"shape\": \"basketball hoop\",\n",
      "          \"label\": \"basketball hoop\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"basketball\",\n",
      "          \"label\": \"basketball\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"bench\",\n",
      "          \"label\": \"bench\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"wall\",\n",
      "          \"label\": \"wall\",\n",
      "          \"notes\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"relationships\": [\n",
      "        \"basketball hoop is above basketball\",\n",
      "        \"bench is to the right of basketball\",\n",
      "        \"wall is behind basketball\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The slide presents the concept of SkillMimic, a method for learning reusable basketball skills through demonstrations. It includes a visual representation of a basketball court with a basketball hoop, a basketball, a bench, and a wall.\"\n",
      "}<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 3] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideAl Weekly\n",
      "\n",
      "Agent Robotics: Learning-from-observation (LfO)\n",
      "一\n",
      "\n",
      "Task model\n",
      "\n",
      "   \n",
      "\n",
      "(Minsky’s frame)\n",
      "\n",
      "Wow-to-do\n",
      "\n",
      "  \n",
      "\n",
      "Source: https://multimodalagentai.github.io/files/Katsushilkeuchi_Talk3_CVPR2024_Tutorial_GeneralistAgentAl.pdf<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"Agent Robotics: Learning-from-observation (LfO)\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Task model\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 2,\n",
      "      \"text\": \"Wow-to-do\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 2,\n",
      "      \"text\": \"Source: https://multimodalagentai.github.io/files/Katsushilkeuchi_Talk3_CVPR2024_Tutorial_GeneralistAgentAl.pdf\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"flowchart\",\n",
      "      \"elements\": [\n",
      "        {\n",
      "          \"shape\": \"cylinder\",\n",
      "          \"label\": \"Task model (Minsky’s frame)\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"Encoding\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"Skill agents\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"What-to-do\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"How-to-do\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"Where-to-do\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"Not direct mimicking\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"Teaching mode\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"Execution mode\",\n",
      "          \"notes\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"relationships\": [\n",
      "        \"Task model (Minsky’s frame) -> Encoding\",\n",
      "        \"Encoding -> Skill agents\",\n",
      "        \"Skill agents -> What-to-do\",\n",
      "        \"Skill agents -> How-to-do\",\n",
      "        \"Skill agents -> Where-to-do\",\n",
      "        \"Skill agents -> Not direct mimicking\",\n",
      "        \"Not direct mimicking -> Teaching mode\",\n",
      "        \"Not direct mimicking -> Execution mode\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The slide presents the concept of Learning-from-observation (LfO) in agent robotics, explaining the process from task modeling to execution mode, emphasizing the importance of not directly mimicking the observed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 4] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideNe <\n",
      "\n",
      "Basic principle of deep reinforcement learning\n",
      "\n",
      "Agent\n",
      "\n",
      "State s                                                               Action a\n",
      "\n",
      "Action\n",
      "\n",
      " \n",
      "\n",
      "Environment<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"Basic principle of deep reinforcement learning\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Agent\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"State s\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Action a\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Action\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"diagram\",\n",
      "      \"elements\": [\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Agent\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"State s\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Action a\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Action\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Environment\",\n",
      "          \"notes\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"relationships\": [\n",
      "        {\n",
      "          \"from\": \"Agent\",\n",
      "          \"to\": \"State s\"\n",
      "        },\n",
      "        {\n",
      "          \"from\": \"Agent\",\n",
      "          \"to\": \"Action a\"\n",
      "        },\n",
      "        {\n",
      "          \"from\": \"Action\",\n",
      "          \"to\": \"Environment\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The slide presents the basic principle of deep reinforcement learning, focusing on the agent, state, action, and environment. It includes a diagram illustrating the interaction between these components.\"\n",
      "}<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 5] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideSkillMimic: Concept\n",
      "\n",
      "@\n",
      "is\n",
      "¢      7\n",
      "\n",
      "一一\n",
      "\n",
      " \n",
      "\n",
      "Motion                                                               SkillMimic\n",
      "—          ee                 Capture\n",
      "\n",
      "tba folate) 41                                HOI Motion Data\n",
      "\n",
      "   \n",
      "\n",
      "Unified learning for all basketball skills<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"SkillMimic: Concept\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Unified learning for all basketball skills\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"diagram\",\n",
      "      \"elements\": [\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Real-World Skill\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Motion Capture\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"HOI Motion Data\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Skill in Simulator\",\n",
      "          \"notes\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"relationships\": [\n",
      "        \"Real-World Skill -> Motion Capture -> HOI Motion Data -> Skill in Simulator\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The slide presents the concept of SkillMimic, which is a unified learning system for basketball skills. It illustrates the process from capturing real-world skills to simulating them in a virtual environment.\"\n",
      "}<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 6] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideSkillMimic: Skill Learning Results\n",
      "\n",
      "Dribble left                                  Dribble turn                                 Dribble back\n",
      "\n",
      " \n",
      "\n",
      "Layup                                           mp shot                               Turnaround layup\n",
      "\n",
      " \n",
      "\n",
      "Rebound                                        Pickup                                            Pass\n",
      "\n",
      " \n",
      "\n",
      "Learning diverse basketball skills within a single policy<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"SkillMimic: Skill Learning Results\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Dribble left\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Dribble turn\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Dribble back\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Layup\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"mp shot\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Turnaround layup\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Rebound\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Pickup\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Pass\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"basketball skills\",\n",
      "      \"elements\": [\n",
      "        {\n",
      "          \"shape\": \"human figure\",\n",
      "          \"label\": \"player\",\n",
      "          \"notes\": \"depicting a basketball player\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"basketball court\",\n",
      "          \"label\": \"court\",\n",
      "          \"notes\": \"depicting a basketball court\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"basketball hoop\",\n",
      "          \"label\": \"hoop\",\n",
      "          \"notes\": \"depicting a basketball hoop\"\n",
      "        }\n",
      "      ],\n",
      "      \"relationships\": [\n",
      "        \"player dribbling on court\",\n",
      "        \"player shooting towards hoop\",\n",
      "        \"player rebounding after shot\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The slide presents various basketball skills such as dribbling, layups, and passing, demonstrating the results of skill learning within a single policy.\"\n",
      "}<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 7] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideiY   This Lecture — Agenda                           (\n",
      "mm\n",
      "\n",
      "= Preliminaries\n",
      "\n",
      "m Generative Models\n",
      "= Transformers\n",
      "= Diffusion Models\n",
      "\n",
      "= Multimodal Embedding Spaces\n",
      "= Al Agent Basics\n",
      "e Acting\n",
      "e Tool Use\n",
      "e Memory<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"This Lecture – Agenda\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Preliminaries\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Generative Models\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Multimodal Embedding Spaces\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"AI Agent Basics\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 2,\n",
      "      \"text\": \"Acting\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 2,\n",
      "      \"text\": \"Tool Use\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 2,\n",
      "      \"text\": \"Memory\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": []\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 8] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideiY   This Lecture — Agenda                              (\n",
      "mm\n",
      "\n",
      "= Preliminaries\n",
      "\n",
      "m Generative Models\n",
      "= Transformers<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"This Lecture – Agenda\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Preliminaries\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Generative Models\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Transformers\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Diffusion Models\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Multimodal Embedding Spaces\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"AI Agent Basics\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Acting\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Tool Use\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Memory\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {},\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": []\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 9] caption:\n",
      "<|im_start|>system<|im_sep|>:\n",
      "You are an AI lecture slide analyzer. The following input is an image and the ocr of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.<|im_end|>\n",
      "\n",
      "<|im_start|>user<|im_sep|>:\n",
      " this is the ocr result of the slideiY    Sequence to Sequence Generation\n",
      "_\n",
      "\n",
      " \n",
      "\n",
      "with Transformer Based Models\n",
      "\n",
      "¢ Input: Sequence x,, ..., X7\n",
      "¢ Output: Sequence yj, ..., yr\n",
      "\n",
      "¢ Transformer is an encoder-decoder architecture:\n",
      "‧ An encoder (neural network) analyzes the input and builds an\n",
      "intermediate representation (a.k.a. context vector) of it;\n",
      "¢ Adecoder (neural network) analyzes the intermediate\n",
      "representation and creates an output.\n",
      "\n",
      "/\n",
      "\n",
      "The house                       Context\n",
      "is red                                   Vector\n",
      "\n",
      "9                                                                                                                  Source: https://huggingface.co/blog/encoder-decoder\n",
      "\n",
      " \n",
      "\n",
      "La casa e\n",
      "rossa<|im_end|>\n",
      "<|im_end|>\n",
      "\n",
      "<|im_start|>assistant<|im_sep|>: \n",
      "{\n",
      "  \"title\": \"Sequence to Sequence Generation with Transformer Based Models\",\n",
      "  \"bullets\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Input: Sequence x1, ..., xT\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Output: Sequence y1, ..., yT'\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"text\": \"Transformer is an encoder-decoder architecture:\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 2,\n",
      "      \"text\": \"An encoder (neural network) analyzes the input and builds an intermediate representation (a.k.a. context vector) of it;\"\n",
      "    },\n",
      "    {\n",
      "      \"level\": 2,\n",
      "      \"text\": \"A decoder (neural network) analyzes the intermediate representation and creates an output.\"\n",
      "    }\n",
      "  ],\n",
      "  \"definitions\": {\n",
      "    \"Transformer\": \"an encoder-decoder architecture\"\n",
      "  },\n",
      "  \"formulas\": [\n",
      "    \"Input: Sequence x1, ..., xT\"\n",
      "  ],\n",
      "  \"examples\": [\n",
      "    \"Output: Sequence y1, ..., yT'\"\n",
      "  ],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"encoder-decoder\",\n",
      "      \"elements\": [\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Encoder\",\n",
      "          \"notes\": \"The house is red\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"arrow\",\n",
      "          \"label\": \"Context Vector\",\n",
      "          \"notes\": \"\"\n",
      "        },\n",
      "        {\n",
      "          \"shape\": \"rectangle\",\n",
      "          \"label\": \"Decoder\",\n",
      "          \"notes\": \"La casa è rossa\"\n",
      "        }\n",
      "      ],\n",
      "      \"relationships\": [\n",
      "        \"The house is red -> Encoder -> Context Vector -> Decoder -> La casa è rossa\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The slide explains the concept of Sequence to Sequence Generation using Transformer Based Models, detailing the input and output sequences, the encoder-decoder architecture, and the process of encoding and decoding.\"\n",
      "}<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m ocr_text \u001b[38;5;241m=\u001b[39m ocr_image(img)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 3. 呼叫 Phi-4\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m caption \u001b[38;5;241m=\u001b[39m \u001b[43mcaption_with_phi4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[頁 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] caption:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcaption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 4. 存檔\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mcaption_with_phi4\u001b[0;34m(img, system, user)\u001b[0m\n\u001b[1;32m     11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimg, text\u001b[38;5;241m=\u001b[39mfull_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processor\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:2116\u001b[0m, in \u001b[0;36mPhi4MMForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, input_mode, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input_mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_image_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_audio_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_audio_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_projection_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_projection_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2135\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1755\u001b[0m, in \u001b[0;36mPhi4MMModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, audio_projection_mode, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1745\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1746\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1752\u001b[0m         cache_position,\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1755\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1459\u001b[0m, in \u001b[0;36mPhi4MMDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m-> 1459\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(attn_outputs)\n\u001b[1;32m   1471\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1236\u001b[0m, in \u001b[0;36mPhi4MMFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;66;03m# Because the input can be padded, the absolute sequence length depends on the max position id.\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m rotary_seq_len \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1236\u001b[0m     \u001b[38;5;28mmax\u001b[39m(kv_seq_len, \u001b[43mposition_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m kv_seq_len\n\u001b[1;32m   1237\u001b[0m )\n\u001b[1;32m   1239\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, seq_len\u001b[38;5;241m=\u001b[39mrotary_seq_len, position_ids\u001b[38;5;241m=\u001b[39mposition_ids)\n\u001b[1;32m   1241\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ocr_image(img: Image.Image) -> str:\n",
    "    return pytesseract.image_to_string(img, lang=\"chi_tra+eng\")\n",
    "\n",
    "def caption_with_phi4(img: Image.Image, system: str, user: str) -> str:\n",
    "    # Build the exact chat-format prompt for Phi-4\n",
    "    full_prompt = (\n",
    "        \"<|im_start|>system<|im_sep|>\"\n",
    "        f\"{system.strip()}\"\n",
    "        \"<|im_end|>\"\n",
    "        \"<|im_start|>user<|im_sep|>\"\n",
    "        f\"{user.strip()}\"\n",
    "        \"<|image_1|>\"           # if your processor expects this token for the image\n",
    "        \"<|im_end|>\"\n",
    "        \"<|im_start|>assistant<|im_sep|>\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(images=img, text=full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=generation_config.max_new_tokens,\n",
    "            num_logits_to_keep=generation_config.num_logits_to_keep,\n",
    "        )\n",
    "    # This will include the assistant’s reply up until the next <|im_end|>\n",
    "    return processor.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "\n",
    "# 逐頁處理並存檔\n",
    "reader = PdfReader(FILE)\n",
    "for page_num in range(1, len(reader.pages) + 1):\n",
    "    # 1. 轉成影像\n",
    "    images = convert_from_path(FILE, dpi=200,\n",
    "                                first_page=page_num, last_page=page_num,\n",
    "                                use_pdftocairo=True)\n",
    "    img = images[0]\n",
    "\n",
    "    # 2. OCR\n",
    "    ocr_text = ocr_image(img)\n",
    "\n",
    "    # 3. 呼叫 Phi-4\n",
    "    caption = caption_with_phi4(img, SYSTEM_PROMPT, ocr_text)\n",
    "    print(f\"[頁 {page_num}] caption:\\n{caption}\\n\")\n",
    "\n",
    "    # 4. 存檔\n",
    "    base = f\"page_{page_num:03d}\"\n",
    "    img.save(os.path.join(output_dir, base + \".png\"))\n",
    "    with open(os.path.join(output_dir, base + \"_caption.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(caption)\n",
    "    with open(os.path.join(output_dir, base + \"_ocr.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(ocr_text)\n",
    "\n",
    "    # 5. 清理\n",
    "    del img, images\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86059362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.34s/it]\n",
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m ocr_text \u001b[38;5;241m=\u001b[39m ocr_image(img)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 3. 呼叫 Phi-4\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m caption \u001b[38;5;241m=\u001b[39m \u001b[43mcaption_with_phi4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[頁 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] caption:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcaption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 4. 存檔\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 40\u001b[0m, in \u001b[0;36mcaption_with_phi4\u001b[0;34m(img, system, user)\u001b[0m\n\u001b[1;32m     38\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimg, text\u001b[38;5;241m=\u001b[39mfull_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 40\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processor\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:2116\u001b[0m, in \u001b[0;36mPhi4MMForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, input_mode, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input_mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_image_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_audio_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_audio_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_projection_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_projection_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2135\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1755\u001b[0m, in \u001b[0;36mPhi4MMModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, audio_projection_mode, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1745\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1746\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1752\u001b[0m         cache_position,\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1755\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1459\u001b[0m, in \u001b[0;36mPhi4MMDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m-> 1459\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(attn_outputs)\n\u001b[1;32m   1471\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1211\u001b[0m, in \u001b[0;36mPhi4MMFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m   1207\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m-> 1211\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m query_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m   1213\u001b[0m query_states \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :query_pos]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:716\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    715\u001b[0m lora_A_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m active_adapter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapters\u001b[49m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m lora_A_keys:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:734\u001b[0m, in \u001b[0;36mBaseTunerLayer.active_adapters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mactive_adapters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;66;03m# is already a list of str\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ocr_image(img: Image.Image) -> str:\n",
    "    return pytesseract.image_to_string(img, lang=\"chi_tra+eng\")\n",
    "\n",
    "def caption_with_phi4(img: Image.Image, system: str, user: str) -> str:\n",
    "    # 把 system 和 user 角色串在一起，並加上 image token\n",
    "    full_prompt = (\n",
    "        \"<|image_1|>\\n\"\n",
    "        \"### System:\\n\" + system.strip() + \"\\n\\n\"\n",
    "        \"### User:\\n\" + user.strip() + \"\\n\\n\"\n",
    "        \"### Assistant:\"\n",
    "    )\n",
    "    inputs = processor(images=img, text=full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=generation_config.max_new_tokens,\n",
    "            num_logits_to_keep=generation_config.num_logits_to_keep,\n",
    "        )\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 逐頁處理並存檔\n",
    "reader = PdfReader(FILE)\n",
    "for page_num in range(1, len(reader.pages) + 1):\n",
    "    # 1. 轉成影像\n",
    "    images = convert_from_path(FILE, dpi=200,\n",
    "                               first_page=page_num, last_page=page_num,\n",
    "                               use_pdftocairo=True)\n",
    "    img = images[0]\n",
    "\n",
    "    # 2. OCR\n",
    "    ocr_text = ocr_image(img)\n",
    "\n",
    "    # 3. 呼叫 Phi-4\n",
    "    caption = caption_with_phi4(img, SYSTEM_PROMPT, ocr_text)\n",
    "    print(f\"[頁 {page_num}] caption:\\n{caption}\\n\")\n",
    "\n",
    "    # 4. 存檔\n",
    "    base = f\"page_{page_num:03d}\"\n",
    "    img.save(os.path.join(output_dir, base + \".png\"))\n",
    "    with open(os.path.join(output_dir, base + \"_caption.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(caption)\n",
    "    with open(os.path.join(output_dir, base + \"_ocr.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(ocr_text)\n",
    "\n",
    "    # 5. 清理\n",
    "    del img, images\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b5fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[頁 1] caption:   \n",
      "Slide page 1:\n",
      "\n",
      "You are an AI lecture slide analyzer. The following input is an image of a lecture slide about “Artificial Intelligence.” \n",
      "1. Extract every piece of written content:\n",
      "   • Slide title\n",
      "   • Section or bullet headings\n",
      "   • Sub-bullets and their full text\n",
      "   • Definitions, formulas, and any inline examples\n",
      "2. Describe every visual element:\n",
      "   • Diagrams or charts (list each shape/box, arrow, label, and the relationship they depict)\n",
      "   • Icons or figures (what they represent and any attached caption)\n",
      "3. Organize your output as a JSON object with these fields:\n",
      "   {\n",
      "     \"title\": string,\n",
      "     \"bullets\": [ { \"level\": int, \"text\": string }, … ],\n",
      "     \"definitions\": { term: definition, … },\n",
      "     \"formulas\": [ string, … ],\n",
      "     \"examples\": [ string, … ],\n",
      "     \"diagrams\": [\n",
      "       {\n",
      "         \"type\": string,\n",
      "         \"elements\": [\n",
      "           { \"shape\": string, \"label\": string, \"notes\": string }, …\n",
      "         ],\n",
      "         \"relationships\": [ string, … ]\n",
      "       }, …\n",
      "     ]\n",
      "   }\n",
      "4. Finally, produce a brief “summary” string of the slide's core message.\n",
      "Be as thorough and precise as possible—this will be used for later retrieval and generation.\n",
      "Example output:\n",
      "{\n",
      "  \"title\": \"Artificial Intelligence\",\n",
      "  \"bullets\": [\n",
      "    { \"level\": 1, \"text\": \"Preliminaries and AI Agent Basics\" },\n",
      "    { \"level\": 2, \"text\": \"AI Agent Basics\" }\n",
      "  ],\n",
      "  \"definitions\": {\n",
      "    \"AI\": \"Artificial Intelligence\"\n",
      "  },\n",
      "  \"formulas\": [],\n",
      "  \"examples\": [],\n",
      "  \"diagrams\": [\n",
      "    {\n",
      "      \"type\": \"network diagram\",\n",
      "      \"elements\": [\n",
      "        { \"shape\": \"circle\", \"label\": \"Node\", \"notes\": \"\" },\n",
      "        { \"shape\": \"arrow\", \"label\": \"Connection\", \"notes\": \"\" }\n",
      "      ],\n",
      "      \"relationships\": [\n",
      "        \"Node A is connected to Node B\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"This slide introduces the basics of AI and AI agents, including a definition of AI and a focus on AI agent basics.\"\n",
      "}\n",
      "[頁 1] image→pdf_pages/page_001.png, caption→pdf_pages/page_001_caption.json, ocr→pdf_pages/page_001_ocr.txt\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labstudent/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <|image_1|> \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSlide page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m PROMPT_TEMPLATE\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 3. Phi-4 Multimodal 產生 JSON 結構化描述\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcaption_with_phi4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[頁 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] caption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 4. OCR 文字擷取\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mcaption_with_phi4\u001b[0;34m(img, prompt)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate caption\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processor\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aicourse/lib/python3.10/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:2116\u001b[0m, in \u001b[0;36mPhi4MMForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, input_mode, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input_mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_image_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_audio_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_audio_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_embed_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_projection_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_projection_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2135\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1755\u001b[0m, in \u001b[0;36mPhi4MMModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, input_image_embeds, image_sizes, image_attention_mask, input_audio_embeds, audio_embed_sizes, audio_attention_mask, audio_projection_mode, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1745\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1746\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1752\u001b[0m         cache_position,\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1755\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1473\u001b[0m, in \u001b[0;36mPhi4MMDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m   1472\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m-> 1473\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1474\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_mlp_dropout(hidden_states)\n\u001b[1;32m   1476\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/modeling_phi4mm.py:1037\u001b[0m, in \u001b[0;36mPhi4MMMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m   1034\u001b[0m gate, up_states \u001b[38;5;241m=\u001b[39m up_states\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1035\u001b[0m up_states \u001b[38;5;241m=\u001b[39m up_states \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(gate)\n\u001b[0;32m-> 1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:712\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m     torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    715\u001b[0m     lora_A_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def caption_with_phi4(img: Image.Image, prompt: str) -> str:\n",
    "    # print(prompt)\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    print(generation_config.num_logits_to_keep)\n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            num_logits_to_keep=generation_config.num_logits_to_keep,\n",
    "            max_new_tokens=generation_config.max_new_tokens,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        \n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def ocr_image(img: Image.Image) -> str:\n",
    "    return pytesseract.image_to_string(img, lang=\"chi_tra+eng\")\n",
    "\n",
    "# ——— 逐頁處理並存檔 ———\n",
    "output_dir = \"pdf_pages\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "reader = PdfReader(FILE)\n",
    "total_pages = len(reader.pages)\n",
    "\n",
    "for page_num in range(1, total_pages + 1):\n",
    "    # 1. 轉成影像\n",
    "    images = convert_from_path(\n",
    "        FILE,\n",
    "        dpi=200,\n",
    "        first_page=page_num,\n",
    "        last_page=page_num,\n",
    "        use_pdftocairo=True\n",
    "    )\n",
    "    img = images[0]\n",
    "\n",
    "    # 2. 分析 prompt：加上頁碼提示\n",
    "    prompt = f\" <|image_1|> \\nSlide page {page_num}:\\n\" + PROMPT_TEMPLATE\n",
    "\n",
    "    # 3. Phi-4 Multimodal 產生 JSON 結構化描述\n",
    "    cap = caption_with_phi4(img, prompt)\n",
    "    print(f\"[頁 {page_num}] caption: {cap}\")\n",
    "    # 4. OCR 文字擷取\n",
    "    txt = ocr_image(img)\n",
    "\n",
    "    # 5. 存檔\n",
    "    img_path = os.path.join(output_dir, f\"page_{page_num:03d}.png\")\n",
    "    cap_path = os.path.join(output_dir, f\"page_{page_num:03d}_caption.json\")\n",
    "    ocr_path = os.path.join(output_dir, f\"page_{page_num:03d}_ocr.txt\")\n",
    "\n",
    "    img.save(img_path, format=\"PNG\")\n",
    "    with open(cap_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cap)  # Phi-4 回傳的 JSON-like text\n",
    "    with open(ocr_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "\n",
    "    print(f\"[頁 {page_num}] image→{img_path}, caption→{cap_path}, ocr→{ocr_path}\")\n",
    "\n",
    "    # 6. 釋放記憶體\n",
    "    del img, images\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d205a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86257ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Load LLM and configure tokenizer\n",
    "print(\"Loading Phi-2 model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\",).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "# Fix pad_token_id warning\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  # e.g., 50256\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Step 4: Process the test query\n",
    "print(f\"Processing query: {QUERY}\")\n",
    "# Retrieve top-5 chunks\n",
    "retrieved_docs = retriever.invoke(QUERY)\n",
    "retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "retrieved_pages = [doc.metadata[\"page\"] for doc in retrieved_docs]\n",
    "\n",
    "# Create prompt\n",
    "prompt = f\"\"\"\n",
    "Query: {QUERY}\n",
    "Retrieved Documents:\n",
    "{chr(10).join([f\"Page {p}: {t}\" for p, t in zip(retrieved_pages, retrieved_texts)])}\n",
    "Instructions: Select the page number (1-463) that directly addresses the query, focusing on technical definitions or metrics. Output only the page number.\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024, padding=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "page_number = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# Validate output\n",
    "try:\n",
    "    page_number = int(page_number)\n",
    "    if not 1 <= page_number <= 463:\n",
    "        raise ValueError\n",
    "except ValueError:\n",
    "    print(\"Invalid page number, falling back to top-ranked chunk\")\n",
    "    page_number = retrieved_pages[0]\n",
    "\n",
    "# Step 5: Output result\n",
    "print(f\"Query ID: {QUERY_ID}\")\n",
    "print(f\"Predicted Page Number: {page_number}\")\n",
    "\n",
    "# Step 6: Save result to CSV (for test case)\n",
    "result = [{\"id\": QUERY_ID, \"page\": page_number}]\n",
    "submission = pd.DataFrame(result)\n",
    "submission.to_csv(\"test_submission.csv\", index=False)\n",
    "print(\"Result saved to test_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define file paths and constants\n",
    "FILE = \"AI.pdf\"  # Path to the 463-page PDF\n",
    "QUERY_FILE = \"HW2_query.csv\"  # Path to query CSV\n",
    "DB_PATH = \"./chroma_db\"  # Path to store Chroma database\n",
    "EMBEDDINGS = \"all-MiniLM-L6-v2\"  # Embedding model\n",
    "OUTPUT_FILE = \"submission.csv\"  # Output CSV for Kaggle\n",
    "\n",
    "# Determine device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Preprocess PDF\n",
    "logger.info(\"Loading and splitting PDF...\")\n",
    "try:\n",
    "    loader = PyPDFLoader(FILE)\n",
    "    pages = loader.load_and_split()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load PDF: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check for empty or problematic pages\n",
    "for i, page in enumerate(pages):\n",
    "    if not page.page_content or len(page.page_content.strip()) < 10:\n",
    "        logger.warning(f\"Page {i+1} has empty or minimal content. Check PDF integrity.\")\n",
    "\n",
    "# Split pages into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(pages)\n",
    "\n",
    "# Verify page metadata\n",
    "for doc in docs:\n",
    "    if \"page\" not in doc.metadata:\n",
    "        logger.error(\"Page number metadata missing in a document chunk\")\n",
    "        raise ValueError(\"Page number metadata missing\")\n",
    "\n",
    "# Step 2: Create vector store\n",
    "logger.info(\"Generating embeddings and creating vector store...\")\n",
    "embedding = HuggingFaceEmbeddings(model_name=EMBEDDINGS)\n",
    "try:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        persist_directory=DB_PATH,\n",
    "        collection_name=\"langchain\"\n",
    "    )\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create vector store: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Load LLM and configure tokenizer\n",
    "logger.info(\"Loading Phi-2 model...\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/phi-2\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load Phi-2 model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Fix pad_token_id warning\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Verify model device\n",
    "logger.info(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Step 4: Load queries\n",
    "logger.info(f\"Loading queries from {QUERY_FILE}...\")\n",
    "try:\n",
    "    queries = pd.read_csv(QUERY_FILE)\n",
    "    query_texts = queries[\"Question\"].tolist()\n",
    "    query_ids = queries[\"ID\"].tolist()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load queries: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Process all queries\n",
    "logger.info(\"Processing queries...\")\n",
    "results = []\n",
    "for query_text, query_id in zip(query_texts, query_ids):\n",
    "    logger.info(f\"Processing query ID: {query_id}\")\n",
    "\n",
    "    # Retrieve top-5 chunks\n",
    "    try:\n",
    "        retrieved_docs = retriever.invoke(query_text)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval failed for query ID {query_id}: {e}\")\n",
    "        results.append({\"id\": query_id, \"page\": 1})  # Fallback page\n",
    "        continue\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        logger.warning(f\"No documents retrieved for query ID {query_id}. Using fallback page.\")\n",
    "        results.append({\"id\": query_id, \"page\": 1})  # Fallback page\n",
    "        continue\n",
    "\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    retrieved_pages = [doc.metadata[\"page\"] for doc in retrieved_docs]\n",
    "    logger.info(f\"Retrieved pages for query ID {query_id}: {retrieved_pages}\")\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"\n",
    "Query: {query_text}\n",
    "Retrieved Documents:\n",
    "{chr(10).join([f\"Page {p}: {t}\" for p, t in zip(retrieved_pages, retrieved_texts)])}\n",
    "Instructions: Select the page number (1-463) that directly addresses the query, focusing on technical definitions or metrics. Output only the page number.\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024, padding=True).to(device)\n",
    "    logger.info(f\"Inputs device for query ID {query_id}: {inputs['input_ids'].device}\")\n",
    "\n",
    "    try:\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        page_number = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Generation failed for query ID {query_id}: {e}\")\n",
    "        page_number = str(retrieved_pages[0])  # Fallback to top-ranked chunk\n",
    "\n",
    "    # Validate output\n",
    "    try:\n",
    "        page_number = int(page_number)\n",
    "        if not 1 <= page_number <= 463:\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        logger.warning(f\"Invalid page number for query ID {query_id}, falling back to top-ranked chunk\")\n",
    "        page_number = retrieved_pages[0]\n",
    "\n",
    "    results.append({\"ID\": query_id, \"Answer\": page_number})\n",
    "\n",
    "# Step 6: Save results to CSV\n",
    "logger.info(f\"Saving results to {OUTPUT_FILE}...\")\n",
    "submission = pd.DataFrame(results)\n",
    "submission.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Results saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4111dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
